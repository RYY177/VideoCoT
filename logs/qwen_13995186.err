/home/avozikis1/my_env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]Fetching 5 files:  20%|██        | 1/5 [01:53<07:34, 113.69s/it]Fetching 5 files:  60%|██████    | 3/5 [01:58<01:02, 31.37s/it] Fetching 5 files:  80%|████████  | 4/5 [01:59<00:20, 20.63s/it]Fetching 5 files: 100%|██████████| 5/5 [01:59<00:00, 23.83s/it]
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.04s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:10<00:15,  5.12s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:15<00:10,  5.08s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:20<00:05,  5.11s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  3.87s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.41s/it]
qwen-vl-utils using decord to read video.
Traceback (most recent call last):
  File "/gpfs/home4/avozikis1/Qwen-2.5/qwen.py", line 219, in <module>
    s = summarize_video(demo_video, style="bullet")
  File "/gpfs/home4/avozikis1/Qwen-2.5/qwen.py", line 170, in summarize_video
    return _run_generation(msgs, max_new_tokens=max_new_tokens, temperature=0.2, do_sample=False)
  File "/gpfs/home4/avozikis1/Qwen-2.5/qwen.py", line 134, in _run_generation
    generated_ids = model.generate(
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/generation/utils.py", line 2522, in generate
    result = self._sample(
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/generation/utils.py", line 3503, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/utils/generic.py", line 959, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1500, in forward
    outputs = self.model(
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1290, in forward
    video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1179, in get_video_features
    video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 483, in forward
    hidden_states = blk(
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/modeling_layers.py", line 95, in __call__
    return super().__call__(*args, **kwargs)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 311, in forward
    hidden_states = hidden_states + self.attn(
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/avozikis1/my_env/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 239, in forward
    query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)
  File "/home/avozikis1/my_env/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 157, in apply_rotary_pos_emb_vision
    k_embed = (k * cos) + (rotate_half(k) * sin)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.02 GiB. GPU 0 has a total capacity of 39.49 GiB of which 1.86 GiB is free. Including non-PyTorch memory, this process has 37.62 GiB memory in use. Of the allocated memory 32.70 GiB is allocated by PyTorch, and 4.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
